{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import traceback\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.gsaelibrary.gsa.gov/ElibMain/contractorList.do?contractorListFor=A\n",
    "\n",
    "basicLink1=\"https://www.gsaelibrary.gsa.gov/ElibMain/\"\n",
    "basicLink2 = \"contractorList.do?contractorListFor=\"   #seperate because basicLink1 is used by sub urls too.\n",
    "pageLetter='A'\n",
    "downloadPath = \"D:\\\\mystuff\\\\Junaid Ahmed\\\\\" \n",
    "bad_chars = \"\\\\/:?*\\\"<>|\"\n",
    "bad_chars_list = [\"\\\\\",\"/\",\":\",\"?\",\"*\",\"\\\"\",\"<\",\">\",\"|\"]\n",
    "\n",
    "dataDictionaryList_FilePath = \"D:\\\\mystuff\\\\Junaid Ahmed\\\\DataDictionary\" \n",
    "gsaList_FilePath = \"D:\\\\mystuff\\\\Junaid Ahmed\\\\gsaList\" \n",
    "noTableList_FilePath = \"D:\\\\mystuff\\\\Junaid Ahmed\\\\noTableList\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already initialized. so commented\n",
    "#dataDictionaryList = []\n",
    "#gsaDictionaryList = []\n",
    "#noTableOrFileDictionaryList = []\n",
    "\n",
    "# link = outer link, list wala page hai ye\n",
    "# subLink = list k har item ka page ka link\n",
    "# linkCounter \n",
    "# fileFinalURL\n",
    "# linkText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveUnwwantedCharacters(myStr):\n",
    "    myStr2 = \"\"\n",
    "    for letter in myStr:\n",
    "        if letter not in bad_chars_list:\n",
    "            myStr2 += letter\n",
    "    return myStr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLink(link):\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        request=requests.get(link)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Connection refused\")\n",
    "        print(\"Sleeping for 5 seconds\")\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            request=requests.get(link)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"Connection refused again.\")\n",
    "            print(\"Sleeping for 15 seconds\")\n",
    "            time.sleep(15)\n",
    "            try:\n",
    "                request=requests.get(link)\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                print(\"Connection refused again.\")\n",
    "                print(\"Sleeping for 30 seconds\")\n",
    "                time.sleep(30)\n",
    "                try:\n",
    "                    request=requests.get(link)\n",
    "                except requests.exceptions.ConnectionError:\n",
    "                    print(\"Connection refused again.\")\n",
    "                    print(\"Sleeping for 60 seconds\")\n",
    "                    time.sleep(60)\n",
    "                    request=requests.get(link)\n",
    "    return request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadFile(url, fileName):\n",
    "    with open(fileName, \"wb\") as file:\n",
    "        response = requests.get(url)\n",
    "        file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link = outer link, list wala page hai ye\n",
    "# subLink = list k har item ka page ka link\n",
    "# linkCounter \n",
    "# fileFinalURL\n",
    "# linkText\n",
    "\n",
    "def UpdateDictionaryList(dictList,dict_var):\n",
    "    dictList.append(dict_var)                    # append doesnt return anything so it will just  append to refrenced list\n",
    "\n",
    "def makeDictionary(link,linkText,linkCounter,subLink,isFileExist, fileFinalURL, contractorInformationHTML):\n",
    "    dataDictionary = {}\n",
    "    dataDictionary[\"Base Link\"] = str(link)\n",
    "    dataDictionary[\"Base Link Text\"] = str(linkText)\n",
    "    dataDictionary[\"Link Number In Base Link\"] = str(linkCounter)\n",
    "    dataDictionary[\"Specific Contract Information Link \"] = str(subLink)\n",
    "    dataDictionary[\"isFileExist\"] = str(isFileExist)\n",
    "    dataDictionary[\"File URL\"] = str(fileFinalURL)\n",
    "    dataDictionary[\"contractor Information Table HTML\"] = str(contractorInformationHTML)\n",
    "    return dataDictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFolderIfNotExist(pageLetter,lastFolderName):\n",
    "    Path(downloadPath+pageLetter+\"\\\\\"+folderName).mkdir(parents=True, exist_ok=True,mode=0o777)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_record(filePath,record):\n",
    "    with open(filePath, 'w') as file:\n",
    "        file.write(json.dumps(record,indent=2)) # use `json.loads` to do the reverse\n",
    "\n",
    "def append_record(filePath,record):\n",
    "    with open(filePath, 'a') as f:\n",
    "        f.write(json.dumps(record, indent=2))\n",
    "        \n",
    "def read_record(filePath):\n",
    "    with open(filePath) as f:\n",
    "        my_list = [json.loads(line) for line in f]\n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "link=basicLink1 + basicLink2 + pageLetter\n",
    "\n",
    "totalTime = 0\n",
    "avgTime = 0\n",
    "\n",
    "r = GetLink(link)\n",
    "    \n",
    "soup=BeautifulSoup(r.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of rows : 571\n",
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, \n",
      " Total links  = 10\n",
      "Total links processed = 10\n",
      "Total elapsed time (in seconds) = 127.11577129364014\n",
      "Average link time (in seconds)  = 12.711577129364013\n",
      "\n",
      " \n",
      "11, 12, 13, 14, 15, 16, 17, 18, 19, 20, \n",
      " Total links  = 20\n",
      "Total links processed = 20\n",
      "Total elapsed time (in seconds) = 155.7317180633545\n",
      "Average link time (in seconds)  = 7.786585903167724\n",
      "\n",
      " \n",
      "21, 22, 23, 24, 25, 26, 27, 28, 29, 30, \n",
      " Total links  = 30\n",
      "Total links processed = 30\n",
      "Total elapsed time (in seconds) = 182.83833646774292\n",
      "Average link time (in seconds)  = 6.0946112155914305\n",
      "\n",
      " \n",
      "31, -----------------------------------------------------------------------------------------\n",
      "There is no file..\n",
      "linkCounter :  31\n",
      "-----------------------------------------------------------------------------------------\n",
      "32, -----------------------------------------------------------------------------------------\n",
      "There is no file..\n",
      "linkCounter :  32\n",
      "-----------------------------------------------------------------------------------------\n",
      "33, -----------------------------------------------------------------------------------------\n",
      "There is no file..\n",
      "linkCounter :  33\n",
      "-----------------------------------------------------------------------------------------\n",
      "34, 35, 36, 37, 38, 39, 40, \n",
      " Total links  = 40\n",
      "Total links processed = 40\n",
      "Total elapsed time (in seconds) = 210.11005997657776\n",
      "Average link time (in seconds)  = 5.252751499414444\n",
      "\n",
      " \n",
      "41, 42, 43, 44, 45, 46, -----------------------------------------------------------------------------------------\n",
      "There is no file..\n",
      "linkCounter :  46\n",
      "-----------------------------------------------------------------------------------------\n",
      "47, -----------------------------------------------------------------------------------------\n",
      "There is no file..\n",
      "linkCounter :  47\n",
      "-----------------------------------------------------------------------------------------\n",
      "48, -----------------------------------------------------------------------------------------\n",
      "There is no file..\n",
      "linkCounter :  48\n",
      "-----------------------------------------------------------------------------------------\n",
      "49, 50, "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rows = soup.find(\"table\", border=1).find_all(\"tr\")\n",
    "\n",
    "linkCounter = 0\n",
    "processedLinkCounter = 0\n",
    "gsaList = []\n",
    "try:\n",
    "    print(\"Total Number of rows : \" + str(len(rows)))\n",
    "    for row in rows:\n",
    "\n",
    "        columns = row.find_all(\"td\")\n",
    "        for column in columns:        \n",
    "\n",
    "            linkCounter+=1\n",
    "            print(linkCounter,end=\", \")\n",
    "            if linkCounter<=0:     #links already processed\n",
    "                continue\n",
    "\n",
    "            processedLinkCounter+=1\n",
    "            startTime = time.time()\n",
    "            linkText = column.get_text()\n",
    "            folderName = RemoveUnwwantedCharacters (linkText)\n",
    "            subURL = row.find_all(\"td\")[0].find('a',href=True)['href']\n",
    "\n",
    "            subLink=basicLink1 + subURL        \n",
    "            subRequest = GetLink(subLink)\n",
    "\n",
    "            subSoup=BeautifulSoup(subRequest.text,'html.parser')\n",
    "            contractorInformationHTML = subSoup.find_all(\"table\")[10]\n",
    "\n",
    "            try:\n",
    "                FileTable = subSoup.find_all(\"table\")[8].find_all(\"table\")[7].find_all(\"tr\")[1].find_all(\"td\")[3].find_all(\"a\")\n",
    "            except exception as e:   #if there is no File table in contractor information\n",
    "                    isFileExist = False\n",
    "                    print(\"-----------------------------------------------------------------------------------------\")\n",
    "                    noTableOrFileDictionaryList.append(linkCounter)\n",
    "                    print(\"There is no table..\")\n",
    "                    print(\"linkCounter : \",linkCounter)\n",
    "                    print(\"-----------------------------------------------------------------------------------------\")\n",
    "                    fileFinalURL =\"No file avaiable\"\n",
    "                    dataDictionary = makeDictionary(link,linkText,linkCounter,subLink,isFileExist, fileFinalURL, contractorInformationHTML)\n",
    "                    UpdateDictionaryList(noTableOrFileDictionaryList,dataDictionary)\n",
    "                    makeFolderIfNotExist(pageLetter,folderName)\n",
    "                    destinationPath = (downloadPath+pageLetter+\"\\\\\"+folderName+\"\\\\\"+\"content.json\")\n",
    "                    write_record(destinationPath,dataDictionary)\n",
    "                    continue\n",
    "            if len(FileTable) == 0:    #if there is no File in contractor information\n",
    "                    isFileExist = False\n",
    "                    print(\"-----------------------------------------------------------------------------------------\")\n",
    "                    noTableOrFileDictionaryList.append(linkCounter)\n",
    "                    print(\"There is no file..\")\n",
    "                    print(\"linkCounter : \",linkCounter)\n",
    "                    print(\"-----------------------------------------------------------------------------------------\")\n",
    "                    fileFinalURL =\"No file avaiable\"\n",
    "                    dataDictionary = makeDictionary(link,linkText,linkCounter,subLink,isFileExist, fileFinalURL, contractorInformationHTML)\n",
    "                    UpdateDictionaryList(noTableOrFileDictionaryList,dataDictionary)                    \n",
    "                    makeFolderIfNotExist(pageLetter,folderName)\n",
    "                    destinationPath = (downloadPath+pageLetter+\"\\\\\"+folderName+\"\\\\\"+\"content.json\")\n",
    "                    write_record(destinationPath,dataDictionary)\n",
    "            else:                      # if there is  atleast one file\n",
    "                \n",
    "                for fileTablePart in FileTable:                    # there can be multiple files in one link so traverse over all of them\n",
    "                    FileHtmURL = fileTablePart[\"href\"]\n",
    "                    fileUrlShort = FileHtmURL.rsplit(\"/\",1)[0]+\"/\"\n",
    "                    if (FileHtmURL == \"http://www.gsa.gov/s2\"):    # some file links are gsa.gov instead of htm links (htm links redirects to either pdf or docx file)\n",
    "                        isFileExist = False\n",
    "                        print(\"-----------------------------------------------------------------------------------------\")\n",
    "                        gsaDictionaryList.append(linkCounter)\n",
    "                        print(\"File link is = http://www.gsa.gov/s2\")\n",
    "                        print(\"linkCounter : \",linkCounter)\n",
    "                        print(\"-----------------------------------------------------------------------------------------\")\n",
    "                        fileFinalURL =\"http://www.gsa.gov/s2\"\n",
    "                        dataDictionary = makeDictionary(link,linkText,linkCounter,subLink,isFileExist, fileFinalURL, contractorInformationHTML)\n",
    "                        UpdateDictionaryList(gsaDictionaryList,dataDictionary)                    \n",
    "                        makeFolderIfNotExist(pageLetter,folderName)\n",
    "                        destinationPath = (downloadPath+pageLetter+\"\\\\\"+folderName+\"\\\\\"+\"content.json\")\n",
    "                        write_record(destinationPath,dataDictionary)\n",
    "                    else:      # file exists and has htm url and that means it will have a file. need to go to htm url and meta tag for actual file url\n",
    "                        isFileExist = True\n",
    "                        fileFinalURL = FileHtmURL\n",
    "                        dataDictionary = makeDictionary(link,linkText,linkCounter,subLink,isFileExist, fileFinalURL, contractorInformationHTML)\n",
    "                        UpdateDictionaryList(dataDictionaryList,dataDictionary)                    \n",
    "                        makeFolderIfNotExist(pageLetter,folderName)\n",
    "                        destinationPath = (downloadPath+pageLetter+\"\\\\\"+folderName+\"\\\\\"+\"content.json\")\n",
    "                        write_record(destinationPath,dataDictionary)\n",
    "\n",
    "            endTime = time.time()\n",
    "            totalTime = totalTime + (endTime-startTime)\n",
    "            avgTime = totalTime/processedLinkCounter\n",
    "\n",
    "            if linkCounter%10==0:\n",
    "                print(\"\\n Total links  = \"+ str(linkCounter))\n",
    "                print(\"Total links processed = \"+ str(processedLinkCounter))\n",
    "                print(\"Total elapsed time (in seconds) = \"+str(totalTime))\n",
    "                print(\"Average link time (in seconds)  = \"+ str(avgTime))\n",
    "                print(\"\\n \")\n",
    "except Exception as e:\n",
    "    print(\"\\n\\n General error occured. \");\n",
    "    print(\"last instances of below items are : \")\n",
    "    print(\"linkCounter = \" + str(linkCounter))\n",
    "    print(\"pageLetter = \" + str(pageLetter))\n",
    "    print(\"link = \" + str(link))\n",
    "    print(\"linkText = \" + str(linkText))\n",
    "    print(\"subLink = \" + str(subLink))\n",
    "    print(\"fileFinalURL = \" + str(fileFinalURL))\n",
    "    print(\"folderName = \" + str(folderName))\n",
    "#     print(\"fileName = \" + str(fileName))\n",
    "    print(\"destinationPath = \" + str(destinationPath))\n",
    "    print(\"Error : \")\n",
    "    print(e)\n",
    "    print(\"Traceback: \")\n",
    "    print(traceback.format_exc())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataDictionaryList = dataDictionaryList[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link=basicLink1 + basicLink2 + pageLetter\n",
    "\n",
    "totalTime = 0\n",
    "avgTime = 0\n",
    "\n",
    "r = GetLink(link)\n",
    "    \n",
    "soup=BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "rows = soup.find(\"table\", border=1).find_all(\"tr\")\n",
    "\n",
    "linkCounter = 0\n",
    "processedLinkCounter = 0\n",
    "gsaList = []\n",
    "try:\n",
    "    print(\"Total Number of rows : \" + str(len(rows)))\n",
    "    for row in rows:\n",
    "\n",
    "        columns = row.find_all(\"td\")\n",
    "        for column in columns:        \n",
    "\n",
    "            linkCounter+=1\n",
    "            print(linkCounter,end=\", \")\n",
    "            if linkCounter<=153:     #links already processed\n",
    "                continue\n",
    "\n",
    "            processedLinkCounter+=1\n",
    "            startTime = time.time()\n",
    "            linkText = column.get_text()\n",
    "            subURL = row.find_all(\"td\")[0].find('a',href=True)['href']\n",
    "\n",
    "            subLink=basicLink1 + subURL        \n",
    "            subRequest = GetLink(subLink)\n",
    "\n",
    "            subSoup=BeautifulSoup(subRequest.text,'html.parser')\n",
    "            try:\n",
    "                FileTable = subSoup.find_all(\"table\")[8].find_all(\"table\")[7].find_all(\"tr\")[1].find_all(\"td\")[3].find_all(\"a\")\n",
    "            except exception as e:   #if there is no file in contractor information\n",
    "                    print(\"-----------------------------------------------------------------------------------------\")\n",
    "                    noTableList.append(linkCounter)\n",
    "                    print(\"There is no file table..\")\n",
    "                    print(\"linkCounter : \",linkCounter)\n",
    "                    print(\"-----------------------------------------------------------------------------------------\")\n",
    "                    \n",
    "                    continue\n",
    "            #its not logging for pages with no files, so run this again afterwards.\n",
    "            for fileTablePart in FileTable:\n",
    "                FileHtmURL = fileTablePart[\"href\"]\n",
    "                fileUrlShort = FileHtmURL.rsplit(\"/\",1)[0]+\"/\"\n",
    "                if (FileHtmURL == \"http://www.gsa.gov/s2\"):\n",
    "                    #not added in dictionary\n",
    "                    print(\"-----------------------------------------------------------------------------------------\")\n",
    "                    gsaList.append(linkCounter)\n",
    "                    print(\"File link is = http://www.gsa.gov/s2 \")\n",
    "                    print(\"linkCounter : \",linkCounter)\n",
    "                    print(\"-----------------------------------------------------------------------------------------\")\n",
    "                else:\n",
    "                    FileHtmRequest = GetLink(FileHtmURL)\n",
    "                    FileHtmSoup =BeautifulSoup(FileHtmRequest.text,'html.parser')\n",
    "                    FileHtmResult = FileHtmSoup.find(\"meta\")[\"content\"]\n",
    "                    FilemetaURL = FileHtmSoup.find(\"meta\")[\"content\"].split(\"url=\")[1]\n",
    "                    fileFinalURL = fileUrlShort+FilemetaURL   \n",
    "\n",
    "                    UpdateDictionaryList(link,subLink,linkCounter, fileFinalURL, linkText)\n",
    "\n",
    "                    folderName = RemoveUnwwantedCharacters (linkText) + \"\\\\\"\n",
    "                    Path(downloadPath+folderName).mkdir(parents=True, exist_ok=True,mode=0o777)\n",
    "                    url = fileFinalURL   \n",
    "                    extension = fileFinalURL.split(\".\")[-1].lower()\n",
    "                    fileName = FileHtmURL.rsplit(\"/\",2)[1] + \".\" +extension\n",
    "                    fileName = RemoveUnwwantedCharacters(fileName)\n",
    "                    destinationPath = (downloadPath+pageLetter+\"\\\\\"+folderName+fileName)\n",
    "                    downloadFile(url, destinationPath)\n",
    "\n",
    "            endTime = time.time()\n",
    "            totalTime = totalTime + (endTime-startTime)\n",
    "            avgTime = totalTime/processedLinkCounter\n",
    "\n",
    "            if linkCounter%10==0:\n",
    "                print(\"\\n\\n Total links  = \"+ str(linkCounter))\n",
    "                print(\"Total links processed = \"+ str(processedLinkCounter))\n",
    "                print(\"Total elapsed time (in seconds) = \"+str(totalTime))\n",
    "                print(\"Average link time (in seconds)  = \"+ str(avgTime))\n",
    "                print(\"\\n\\n \")\n",
    "except Exception as e:\n",
    "    print(\"\\n\\n General error occured. \");\n",
    "    print(\"last instances of below items are : \")\n",
    "    print(\"linkCounter = \" + str(linkCounter))\n",
    "    print(\"pageLetter = \" + str(pageLetter))\n",
    "    print(\"link = \" + str(link))\n",
    "    print(\"linkText = \" + str(linkText))\n",
    "    print(\"subLink = \" + str(subLink))\n",
    "    print(\"fileFinalURL = \" + str(fileFinalURL))\n",
    "    print(\"folderName = \" + str(folderName))\n",
    "    print(\"fileName = \" + str(fileName))\n",
    "    print(\"destinationPath = \" + str(destinationPath))\n",
    "    print(\"Error : \")\n",
    "    print(e)\n",
    "    print(\"Traceback: \")\n",
    "    print(traceback.format_exc())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These pages are unique and have no table\n",
    "# ---------------------------------------\n",
    "#linkCounter = 151\n",
    "#pageLetter = B\n",
    "#sublink = https://www.gsaelibrary.gsa.gov/ElibMain/SearchResults;jsessionid=SQEyLT4aQw7OriGhJJunmNZo.prd2pweb64?searchType=exactWords&coSearch=Y&searchText=BALL+AEROSPACE+%26+TECHNOLOGIES+CORP.\n",
    "# ---------------------------------------\n",
    "#linkCounter = 152\n",
    "#pageLetter = B\n",
    "#sublink = https://www.gsaelibrary.gsa.gov/ElibMain/SearchResults;jsessionid=eKdbFPm3lrkurrKCqjGxtzr4.prd2pweb64?searchType=exactWords&coSearch=Y&searchText=BALL+AEROSPACE+%26+TECHNOLOGIES+CORP.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
